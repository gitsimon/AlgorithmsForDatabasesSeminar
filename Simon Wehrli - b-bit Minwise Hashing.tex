\documentclass[a4paper]{article}
\author{Simon Wehrli}
\date{28.4.2013}
\title{Set similarity with b-bit k-permutation Minwise Hashing - DRAFT}

\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{tabulary}
\usepackage{caption}
\usepackage[english]{varioref} %automatische Anpassung von Referenzen
\usepackage{float} % bewirkt, dass Option [H] f√ºr float-Umgebungen von Latex umgesetzt wird

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newtheorem{mydef}{Definition}
\newtheorem{mylemma}{Lemma}

\begin{document} 
\maketitle

\section{Introduction}
This report is written for the seminar titled ``Algorithms for Database Systems" at ETH Z\"{u}rich. The seminar participants read and summarize various papers, which treat solving problems in the context of Big Data, which is this year's topic.

With Big Data, most problems emphasis shifts from computational complexity to memory space usage considerations. Typically, the runtime is dominated by the time needed for memory accesses, hence optimizing the memory footprint already improves our algorithms significantly. 

Many today's applications are faced with very large datasets. A common task is to find \emph{similarity} between two or several such sets. There are lots of problem solutions which use a mapping to sets of properties and then do a \emph{similarity search} on them. Fast (approximative) algorithms for this search enable improvements of many well-known algorithms, e.g. of machine learning and computer vision.

\subsection{Notation and Terminology}
We denote by $\Omega$ the set of all possible items of the sets $S_i \subseteq \Omega$, $i = 1 \text{ to } n$. $\left| \Omega \right| = D$ is always large (e.g. $D=2^{64}$) and would require prohibitive storage. Often we consider only two sets $S_1 = X$, $S_2 = Y$. The normalized similarity between $X$ and $Y$, known as \emph{resemblance} or \emph{Jaccard similarity}, denoted by R, is
\[
R=\frac{\left| X \cap Y \right|}{\left| X \cup Y \right|} = \frac{a}{\left| X \right| + \left| Y \right| -a}, \qquad \text{where } a=\left| X \cap Y \right|
\]
We use $\Pr[\cdot]$ as a shorthand for the \emph{Probability} of some event.

\subsection{Motivating Example}
Consider a web search provider, which want to present a result list of web pages without duplicates. To achieve that, for every pair of web pages, we drop one of them, if they are textually very similar. This is the case when their \emph{resemblance} $R$ is greater than some threshold $R_0$. But to be able to use the \emph{resemblance} as a measurement, we have to map each page to a set. One could imagine to define this mapping from the page to the set of all words occurring on the page. As in several studies \citep{Broder:1998,BroderGMZ97} we will instead map a page to a set of \emph{shingles}. A shingle is a string of $w$ contiguous words, and we include a \emph{shingle} in our result set if the \emph{shingle} occurs on the page (in the same order). Typically we choose $w = 5$. Figure \vref{fig:shingle} shows an exemplary mapping.

\begin{figure}[h]
\begin{center}
\fbox{
\parbox{2cm}{
This web page is just an example.
}
}
\hspace{1cm} $\underrightarrow{\text{mapping}}$ \hspace{1cm}
\parbox{2cm}{
\begin{equation*}
\begin{split}
 \text{\{} & \text{"This web page"},\\ & \text{"web page is"},\\ & \text{"page is just"},\\ & \text{"is just an"},\\ & \text{"just an example"} \text{\}}
\end{split}
\end{equation*}
}
\end{center}
\caption{The mapping from a web page to a set of \emph{shingles}.}
\label{fig:shingle}
\end{figure}

Clearly, the number of possible shingles and therefore $D$ is huge. Assume $10^5$ different English words, we have $D=\left(10^5\right)^5 \in \Theta\left(2^{25}\right)$. Thus computing the exact similarities for all pairs of pages of a web search would require prohibitive storage. For a single pair, we would already need $\Theta(D)$ storage.

\subsection{Original Minwise Hashing}

\section{b-bit k-permutation Minwise Hashing}

\begin{algorithm}[H]
\caption{\textsc{b-bit Minwise Hashing} algorithm, applied to estimating pairwise resemblances in a collection of $n$ sets.}
\label{MinwiseHashingAlg}
\begin{algorithmic}
\Require Sets $S_i \subseteq \Omega = \{0,1,\ldots,D-1\}, i = 1 \text{ to } n$. \Comment $D = \left| \Omega \right|$
\Ensure Estimated resemblance $\hat{R}_b$
\State // Pre-processing
\State Generate $k$ random permutations $\pi_j: \Omega\longrightarrow\Omega, j=1\text{ to }k$
\ForAll{$i = 1 \text{ to } n, j = 1 \text{ to } k$}
	\State Store the lowest $b$ bits of $\min(\pi_j(S_i))$, denoted by $e_{S_i,b,\pi_j}$.
\EndFor
\State
\State // Estimation (Use two sets $X,Y$ as an example)
\State Compute $\hat{P}_b = \frac{1}{k}\sum_{j=1}^k 1 \left\lbrace  e_{X,b,\pi_j } = e_{Y,b,\pi_j } \right\rbrace$
\State Estimate the resemblance by $\hat{R}_b = \frac{\hat{P}_b-F_{\text{pos}}}{1-F_{\text{neg}}}$
\end{algorithmic}
\end{algorithm}


\subsection{Fundamental Results}

\subsection{Experiments}


%\begin{center}
\begin{table}
{\scriptsize 
\begin{tabulary}{1\textwidth}{LLL}
%\begin{tabular}{|l|l|l|l|l|}
\hline \textbf{Algorithm}  %& \textbf{Type} & \textbf{Space complexity} 
& \textbf{Pros} & \textbf{Cons} \\ 
\hline Karp, Shenker %& Deterministic %& $O\left(\frac{1}{s}\right)$ 
& Low memory usage.\newline Constant amortized time complexity per element. & No guarantees regarding false positives.\newline Requires two passes to find I(x,s) exactly.\newline Does not handle deletes.\newline Singleton streams only. \\ 
\hline Manku, Motwani (StickySampling) %& Randomized 	%& $O\left(\frac{1}{\varepsilon}\log \left(\frac{1}{s \cdot \delta}\right)\right)$ 
& Strong guarantees. & Worse average space complexity than LossyCounting.\newline Does not handle deletes. \\ 
\hline Manku, Motwani (LossyCounting) %& Deterministic %& $O\left(\frac{1}{\varepsilon}\log (\varepsilon \cdot n)\right)$ 
& Strong guarantees.\newline Good average space complexity.\newline Can be extended to handle itemsets.  & Does not handle deletes. \\ 
\hline Cormode, Muthukrishnan  %& Randomized 	%& $O\left(\frac{1}{s} \log\left(\frac{1}{s\cdot \delta}\right) \log m\right)$ 
& Handles deletes.\newline Moderate guarantees. & Singleton streams only.\newline False negatives possible.\newline Requires bijection to integers. \\ 
\hline 
%\end{tabular} 

\end{tabulary}
}
\caption{Algorithm comparison}
\label{ComparisonTable}
\end{table}
%\end{center}

\section{Comparison with Hamming Distance Algorithms}


\section{Conclusions}


\bibliographystyle{unsrt}
\bibliography{Bibliography}

\end{document}